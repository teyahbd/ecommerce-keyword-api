{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Two: Training the Model\n",
    "\n",
    "After preparing our dataset into a corpus - a collection of sentences - we can train our model and create our custom word embeddings. This method uses the dataset created using the steps outlined in ```clean_data.ipynb```. However, this process should also work for any corpus formatted as a list of sentences in a text file. \n",
    "\n",
    "To create our word embeddings, we will make use of Word2Vec algorithms (see ```README.md``` for this directory for further details). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "- We will use the ```Gensim``` library - a standard in natural language processing projects. More specifically, the [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) module provides functions which will allow use to train and make use of our word embeddings. \n",
    "- We need a text corpus formatted as a text file containing a list of sentences to evaluate. \n",
    "\n",
    "\n",
    "Note: As mentioned previously, it would be most appropriate to create a new directory and virtual environment for training your data instead of within this API directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Training the Model\n",
    "\n",
    "We can now create our ```Word2Vec``` neural network-based model! The code below is deceptively simple as it contains a massive amount of functionality. By inputting our corpus at the start when initialising the model, the steps of building vocabulary and training the model can be completed in one go. This can also be done a step at a time, see the [documentation here](https://radimrehurek.com/gensim/models/word2vec.html) for examples. \n",
    "\n",
    "Additionally, there are many parameters that can be set to control the features and implementation of the model. For example, you can select between the *skip-gram* or the *CBOW (continuous bag of words)* algorithms. While the many options are certainly worth playing with, for now we will keep it simple. The one optional parameter we will set is ```min_count```. This sets the frequency below which all less-frequent words be ignored. As our dataset is relatively small, we will set this at 1 so as many words as possible are included in our word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(min_count=1, corpus_file=\"./cleaned_dataset.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Word Vectors\n",
    "\n",
    "We have our model! We can do things such as save or load this model, but for now we want to access our custom trained word vectors. Since we've finished the training, we no longer need the full model state. We can extract the vectors and keys alone - a much smaller and faster object than the full model - to use in our API functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Word Vectors\n",
    "\n",
    "Now we have our word embeddings, we can use them to explore the relationships between words from our dataset! The most important functionality for our use is finding similar keywords for a given word. Let's try to find the 10 words most similar to the word **heart**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to heart are [('star', 0.6312820315361023), ('stars', 0.4272494316101074), ('tealight', 0.4014790952205658), ('chandelier', 0.376049667596817), ('chick', 0.3455809950828552), ('bird', 0.34482234716415405), ('candle', 0.342176228761673), ('candelabra', 0.3411673307418823), ('soap', 0.3409248888492584), ('seat', 0.3406926691532135)]\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar(\"heart\")\n",
    "\n",
    "print(\"Words most similar to heart are\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create more complex queries. We can use multiple keywords to find words most similar to two or more keywords. Let's find words most similar to both **heart** and **bag**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to heart and bag are [('star', 0.4480481445789337), ('bunting', 0.3603987991809845), ('box', 0.3536360561847687), ('wrap', 0.3321145176887512), ('tape', 0.32946687936782837), ('seat', 0.30161723494529724), ('sack', 0.292302668094635), ('chick', 0.2723071575164795), ('tags', 0.2681472897529602), ('jug', 0.26459288597106934)]\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar([\"heart\",\"bag\"])\n",
    "\n",
    "print(\"Words most similar to heart and bag are\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also input \"negative\" keywords - which contribute negatively to the similarity. For example, if we want keywords similar to **heart** but not similar to **star**, we can input an array of positive words and an array of negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to heart but not to star are [('candle', 0.3686973452568054), ('sweetheart', 0.3085654377937317), ('pan', 0.29118064045906067), ('folding', 0.28588226437568665), ('buddha', 0.26655375957489014), ('photoframe', 0.26415762305259705), ('shape', 0.2628423869609833), ('cosy', 0.26227304339408875), ('diner', 0.25945886969566345), ('eau', 0.251926064491272)]\n"
     ]
    }
   ],
   "source": [
    "positive_words = [\"heart\"]\n",
    "negative_words = [\"star\"]\n",
    "\n",
    "result = word_vectors.most_similar(positive=positive_words, negative=negative_words)\n",
    "\n",
    "print(\"Words most similar to heart but not to star are\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the positive and negative arrays can contain multiple keywords. See the [KeyedVectors module documentation](https://radimrehurek.com/gensim/models/keyedvectors.html) for further details on this functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading the Word Vectors\n",
    "\n",
    "To make the most of our word vectors, we want to store and load them for use at a later date. We can do this by storing them as a text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.save_word2vec_format('ecommerce_vecs.txt', binary= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load them from the text file at a later point wherever we want! We require the module ```KeyedVectors``` from ```Gensim``` in whatever file we wish to use our word vectors within. Then, we simply load them from the text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words most similar to heart are [('star', 0.6312820315361023), ('stars', 0.4272494316101074), ('tealight', 0.4014790952205658), ('chandelier', 0.376049667596817), ('chick', 0.3455809950828552), ('bird', 0.34482234716415405), ('candle', 0.342176228761673), ('candelabra', 0.3411673307418823), ('soap', 0.3409248888492584), ('seat', 0.3406926691532135)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"ecommerce_vecs.txt\", binary=False) \n",
    "\n",
    "result = word_vectors.most_similar(\"heart\")\n",
    "\n",
    "print(\"The words most similar to heart are\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our keyword API, we can now load our word vectors into our ```model.py``` and create functionality that accepts keywords in a POST request and returns similar words for future product searches. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
