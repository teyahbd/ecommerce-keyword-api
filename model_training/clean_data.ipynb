{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step One: Cleaning the Dataset\n",
    "\n",
    "It is necessary to preprocess our data to get it into an appropriate and useful format for training our word vectors. To create custom trained word vectors, we need a corpus - a collection of sentences (or product descriptions in our case). Using these sentences, our training will create relationships between each of the words and quantify these as word vectors (or word embeddings).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "- We are using an online retail dataset from the [UCI machine learning repository](https://archive.ics.uci.edu/ml/index.php) which can be downloaded [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00352/).\n",
    "\n",
    "- We will clean and train our data in Python - the programming language of choice for many machine learning and data science projects. We will make use of the popular `pandas` library to manipulate our dataset.\n",
    "\n",
    "Note: The training files are stored in this repo for convenience however, it would be most appropriate to create a new directory and virtual environment to the API directory and follow these steps there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data\n",
    "\n",
    "Our data is formatted as an Excel file, so we can read it into a `pandas DataFrame`. As mentioned, we only need the \"Description\" column for our use so will only import this column.\n",
    "\n",
    "As our dataset is large, this may take a few minutes. Once this is completed, we should see the first few rows of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Description\n",
       "0   WHITE HANGING HEART T-LIGHT HOLDER\n",
       "1                  WHITE METAL LANTERN\n",
       "2       CREAM CUPID HEARTS COAT HANGER\n",
       "3  KNITTED UNION FLAG HOT WATER BOTTLE\n",
       "4       RED WOOLLY HOTTIE WHITE HEART."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('Online_Retail.xlsx', usecols=\"C\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data\n",
    "\n",
    "As our dataset has over half a million lines, it would not be practical to manually check for missing or nonsensical entries. Therefore, we must find ways to manipulate and clean the dataset as a whole so our model will be as useful as possible. This was a trial and error process specific to this dataset and there are likely further steps that could be added.\n",
    "\n",
    "#### 1. Remove any empty rows\n",
    "\n",
    "There are some empty rows in our dataset, which we can check for. Make sure you run this block _before_ removing the empty rows!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty rows before cleaning: Description    1454\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Empty rows before cleaning:\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can remove these empty rows and check there are no null valued rows remaining. Make sure to modify the ```DataFrame``` in place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty rows after cleaning: Description    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"Empty rows after cleaning:\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove non-product descriptions\n",
    "\n",
    "The next step in cleaning our data involves checking the contents of the sentences. After a brief look at the dataset, it was clear that some product descriptions indicated a product was missing or damaged. These lines would not be useful in our model as we are looking for useful keywords to use in search queries for related products. However, it is important to find the balance between removing problematic data and retaining as large a dataset as possible.\n",
    "\n",
    "We found that these irrelevant lines often contained the following terms:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before cleaning: 540455\n"
     ]
    }
   ],
   "source": [
    "bad_lines = \"\\?|damaged|damages|damage\"\n",
    "\n",
    "print(\"Rows before cleaning:\", df.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now manipulate our DataFrame object to drop any rows containing our unwanted terms. We can see at least 1000 lines have been removed (so long as you haven't already run this block!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 540269\n"
     ]
    }
   ],
   "source": [
    "df = df[df[\"Description\"].str.contains(bad_lines) == False]\n",
    "\n",
    "print(\"Rows after cleaning:\", df.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove unhelpful keywords\n",
    "\n",
    "We've removed rows containing unhelpful product descriptions, but what about unhelpful keywords? On evaluating the dataset again, it was clear that many descriptions contained both relevant description and irrelevant or nonsensical words or punctuation. It would be wasteful to remove thousands more lines from our dataset however, bad keywords could reduce the usefulness of our model. \n",
    "\n",
    "In our case, we will remove any problematic words or punctuation from a line while retaining the relevant product description. This strikes a good balance between dataset size and usefulness. \n",
    "\n",
    "After considering examples from the data, our method is to: \n",
    "\n",
    "- remove words with less than 3 characters\n",
    "- remove words containing \"?\" or \"/\"\n",
    "- remove punctuation (\".\", \",\", \"\"\") from words then add the word back to the sentence\n",
    "\n",
    "\n",
    "While this would likely be possible using ```DataFrame```, we will convert this to a text file to do this last part of cleaning. Conveniently, our final output will need to be a text file so this step would always have been necessary. \n",
    "\n",
    "Notes: \n",
    "- For clarity, we will convert our ```DataFrame``` to a text file separately to removing bad keywords however, they could be done in one step. \n",
    "- We convert the text to lowercase at this point so our word embeddings are in the most useful format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"dataset.txt\", \"w\")\n",
    "for data in df.columns:\n",
    "    text_file.write((df[data].to_string(index=False)+'\\n').lower())\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement our method and remove or replace any remaining problematic keywords. Here, we delete the previous text file, but you can remove the first and last lines if you would like to keep both for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bad_chars = [\"?\", \"/\"]\n",
    "punct = [\".\", \",\", \"\\\"\"]\n",
    "\n",
    "with open('dataset.txt') as oldfile, open('cleaned_dataset.txt', 'w') as newfile:\n",
    "    for line in oldfile:\n",
    "        clean_arr = []\n",
    "        sentence = line.split()\n",
    "        for word in sentence:\n",
    "            if not any(chars in word for chars in bad_chars):\n",
    "                if len(word) > 2:\n",
    "                    if any(chars in word for chars in punct):\n",
    "                        clean_word = word\n",
    "                        for item in punct:\n",
    "                            clean_word = clean_word.replace(item, \"\")\n",
    "                        clean_arr.append(clean_word)\n",
    "\n",
    "                    else: \n",
    "                        clean_arr.append(word)\n",
    "        clean_str = ' '.join(clean_arr) +'\\n'\n",
    "        newfile.write(clean_str)\n",
    "\n",
    "os.remove(\"dataset.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have our cleaned dataset ```cleaned_dataset.txt``` in an appropriate corpus format! We can now make use of Word2Vec and begin training our word embeddings. Check out ```model_training.ipynb``` for the next step!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
